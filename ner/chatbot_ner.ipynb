{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read recipes csv file\n",
    "df = pd.read_csv('recipes.csv')\n",
    "#read ingredients csv file\n",
    "df2 = pd.read_csv('ingredients.csv')\n",
    "#read filtered tags csv file\n",
    "df3 = pd.read_csv('filtered_tags.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list\n",
    "TRAIN_DATA = [\n",
    "    (\"عايز وصفة فراخ مقليه\", {\n",
    "        'entities': [(10, 20, 'Recipe')]\n",
    "    }),\n",
    "    (\"عايز اكل مكرونه بشاميل\", {\n",
    "        'entities': [(9, 22, 'Recipe')]}),\n",
    "    (\"ممكن تعمللي مكرونه بشاميل\", { \n",
    "        'entities': [(12, 25, 'Recipe')]}),\n",
    "    (\"متعملنا سندوتش برجر\", {\n",
    "        'entities': [(8, 19, 'Recipe')]}),\n",
    "    (\"عايز اكلة زي المكرونة بشاميل\", {\n",
    "        'entities': [(13, 28, 'Recipe')]}),\n",
    "    (\"عايز اكلة شبه النجرسكو\", {\n",
    "        'entities': [(14, 22, 'Recipe')]}),\n",
    "    (\"عايز اكلة زي المكرونة بشاميل\", {\n",
    "        'entities': [(13, 28, 'Recipe')]}),\n",
    "    (\"عايز اكل اكله زى مكرونه بشاميل\", {\n",
    "        'entities': [(17, 31, 'Recipe')]}),\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterator in range(8):\n",
    "    x = TRAIN_DATA[iterator][1]\n",
    "    x= x[\"entities\"]\n",
    "    i = x[0][0]\n",
    "    j = x[0][1]\n",
    "    for iter in range(500):\n",
    "        # get a random recipe from recipes.csv\n",
    "        recipe = df.sample(n=1)\n",
    "        recipe = recipe[\"recipes\"]\n",
    "        # read name only\n",
    "        recipe = recipe.values[0]\n",
    "        new_entry = TRAIN_DATA[iterator][0][:i] + recipe + TRAIN_DATA[iterator][0][j:]\n",
    "        print(new_entry)\n",
    "        # get new i and j\n",
    "        i_new = i\n",
    "        j_new = i + len(recipe)\n",
    "        # add in train data\n",
    "        TRAIN_DATA.append((new_entry, {\n",
    "            'entities': [(i_new, j_new, 'Recipe')]\n",
    "        }))\n",
    "\n",
    "# export to json file\n",
    "df = pd.DataFrame(TRAIN_DATA, columns=['text', 'entities'])\n",
    "with open('df.json', 'w', encoding='utf-8') as file:\n",
    "    df.to_json(file, force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_2 = [\n",
    "    (\"عايز آكل حاجة صحية و فيها فراخ\", {\n",
    "        'entities': [(26, 30, 'Ingredient')]\n",
    "    }),\n",
    "    (\"عايز اكله فيها فراخ\", {\n",
    "        'entities': [(15, 19, 'Ingredient')]}),\n",
    "    (\"عايز اكله انهاردة فيها رز\", {\n",
    "        'entities': [(23, 25, 'Ingredient')]}),\n",
    "    (\"ممكن اكله فيها فراخ و تكون صحية\", {\n",
    "        'entities': [(15, 19, 'Ingredient')]}),\n",
    "    (\"نفسي ف حاجة فيها مكرونة\", {\n",
    "        'entities': [(17, 23, 'Ingredient')]}),\n",
    "    (\" قولي اكلة بالفراخ\", {\n",
    "        'entities': [(12, 18, 'Ingredient')]}),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterator in range(6):\n",
    "    x = TRAIN_DATA_2[iterator][1]\n",
    "    x= x[\"entities\"]\n",
    "    i = x[0][0]\n",
    "    j = x[0][1]\n",
    "    for iter in range(500):\n",
    "        # get a random ingredient from ingredients.csv\n",
    "        ingredient = df2.sample(n=1)\n",
    "        ingredient = ingredient[\"ingredient\"]\n",
    "        # read name only\n",
    "        ingredient = ingredient.values[0]\n",
    "        new_entry = TRAIN_DATA_2[iterator][0][:i] + ingredient + TRAIN_DATA_2[iterator][0][j:]\n",
    "        print(new_entry)\n",
    "        # get new i and j\n",
    "        i_new = i\n",
    "        j_new = i + len(ingredient)\n",
    "        # add in train data\n",
    "        TRAIN_DATA_2.append((new_entry, {\n",
    "            'entities': [(i_new, j_new, 'Ingredient')]\n",
    "        }))\n",
    "# export to json file\n",
    "df2 = pd.DataFrame(TRAIN_DATA_2, columns=['text', 'entities'])\n",
    "with open('df2.json', 'w', encoding='utf-8') as file:\n",
    "    df2.to_json(file, force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_3 = [\n",
    "    (\"عايز آكل حاجة صحية و فيها بروتين\", {\n",
    "        'entities': [(14, 18, 'Tag')]\n",
    "    }),\n",
    "    (\"عايز وصفة أكل سهلة و سريعة\", {\n",
    "        'entities': [(14, 18, 'Tag'), (21, 25, 'Tag')]}),\n",
    "    (\"عايزة اكله سريعة\", {\n",
    "        'entities': [(11, 16, 'Tag')]}),\n",
    "    (\"عايزة اكلة صحية\", {\n",
    "        'entities': [(11, 15, 'Tag')]}),\n",
    "    (\"عايز انهاردة وجبة خفيفة كدا\", {\n",
    "        'entities': [(18, 23, 'Tag')]}),\n",
    "    (\"ممكن اكله فيها فراخ و تكون صحية\", {\n",
    "        'entities': [(27, 31, 'Tag')]}),\n",
    "    (\"عايز وصفة أكل سريعة و سهلة عشان مستعجل\", {\n",
    "        'entities': [(14, 19, 'Tag'), (22, 25, 'Tag')]}),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterator in range(7):\n",
    "    x = TRAIN_DATA_3[iterator][1]\n",
    "    x= x[\"entities\"]\n",
    "    i = x[0][0]\n",
    "    j = x[0][1]\n",
    "    for iter in range(50):\n",
    "        # get a random ingredient from ingredients.csv\n",
    "        tag = df3.sample(n=1)\n",
    "        tag = tag[\"tag\"]\n",
    "        # read name only\n",
    "        tag = tag.values[0]\n",
    "        new_entry = TRAIN_DATA_3[iterator][0][:i] + tag + TRAIN_DATA_3[iterator][0][j:]\n",
    "        print(new_entry)\n",
    "        # get new i and j\n",
    "        i_new = i\n",
    "        j_new = i + len(tag)\n",
    "        # add in train data\n",
    "        TRAIN_DATA_3.append((new_entry, {\n",
    "            'entities': [(i_new, j_new, 'Tag')]\n",
    "        }))\n",
    "# export to json file\n",
    "df3 = pd.DataFrame(TRAIN_DATA_3, columns=['text', 'entities'])\n",
    "with open('df3.json', 'w', encoding='utf-8') as file:\n",
    "    df3.to_json(file, force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join TRAIN_DATA and TRAIN_DATA_2\n",
    "TRAIN_DATA = TRAIN_DATA + TRAIN_DATA_2 + TRAIN_DATA_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "output_dir=Path(\"C:/Users/Fastora/Documents/GitHub/ChefwNos/ner/ner_model\")\n",
    "n_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "#set up the pipeline\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "            nlp.update(\n",
    "                [example],  \n",
    "                drop=0.5,  \n",
    "                sgd=optimizer,\n",
    "                losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from C:\\Users\\Fastora\\Documents\\GitHub\\ChefwNos\\ner\\ner_model\n",
      "Entities [('جمبري و مكرونة', 'Ingredient')]\n",
      "Tokens [('نفسي', '', 2), ('في', '', 2), ('حاجة', '', 2), ('فيها', '', 2), ('جمبري', 'Ingredient', 3), ('و', 'Ingredient', 1), ('مكرونة', 'Ingredient', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "\n",
    "doc = nlp2(\"نفسي في حاجة فيها جمبري و مكرونة\")\n",
    "print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
