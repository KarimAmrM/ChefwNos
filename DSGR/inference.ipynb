{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\Fastora\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from network import DSGR\n",
    "import dgl\n",
    "from dgl import load_graphs\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = './data/recipes50_graph'\n",
    "graph = load_graphs(graph_path)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/recipes50.csv')\n",
    "\n",
    "\n",
    "#get user with user_id = 14326\n",
    "user_id = 14326\n",
    "user = df[df['user_id'] == user_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes:  3\n",
      "Items:  [4942 2293 9323]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_data = user.sort_values(['time'], kind='mergesort')\n",
    "print('Number of recipes: ', len(user_data))\n",
    "\n",
    "#print the items_id\n",
    "print('Items: ', user_data['item_id'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = 14326\n",
    "test_items = user_data['item_id'].values\n",
    "test_times = user_data['time'].values\n",
    "\n",
    "\n",
    "graphs = []\n",
    "for j, t in enumerate(test_times[0:-1]):\n",
    "    if j == 0:\n",
    "        continue\n",
    "    if j < 50:\n",
    "        start_t = test_times[0]\n",
    "    else:\n",
    "        start_t = test_times[j - 50]\n",
    "    sub_u_eid = (graph.edges['by'].data['time'] < test_times[j + 1]) & (graph.edges['by'].data['time'] >= start_t)\n",
    "    sub_i_eid = (graph.edges['pby'].data['time'] < test_times[j + 1]) & (graph.edges['pby'].data['time'] >= start_t)\n",
    "    sub_graph = dgl.edge_subgraph(graph, edges={'by': sub_u_eid, 'pby': sub_i_eid}, relabel_nodes=False)\n",
    "    u_temp = torch.tensor([test_user])\n",
    "    his_user = torch.tensor([test_user])\n",
    "    graph_i =  dgl.sampling.select_topk(sub_graph,50, weight='time', nodes={'user': u_temp})\n",
    "    i_temp = torch.unique(graph_i.edges(etype='by')[0])\n",
    "    his_item = torch.unique(graph_i.edges(etype='by')[0])\n",
    "    edge_i = [graph_i.edges['by'].data[dgl.NID]]\n",
    "    edge_u = []\n",
    "    for _ in range(3-1):\n",
    "        graph_u = dgl.sampling.select_topk(sub_graph, 50, weight='time', nodes={'item': i_temp})  # item的邻居user\n",
    "        u_temp = np.setdiff1d(torch.unique(graph_u.edges(etype='pby')[0]), his_user)[-50:]\n",
    "        #u_temp = torch.unique(torch.cat((u_temp, graph_u.edges(etype='pby')[0])))\n",
    "        graph_i = dgl.sampling.select_topk(sub_graph, 50, weight='time', nodes={'user': u_temp})\n",
    "        his_user = torch.unique(torch.cat([torch.tensor(u_temp), his_user]))\n",
    "        #i_temp = torch.unique(torch.cat((i_temp, graph_i.edges(etype='by')[0])))\n",
    "        i_temp = np.setdiff1d(torch.unique(graph_i.edges(etype='by')[0]), his_item)\n",
    "        his_item = torch.unique(torch.cat([torch.tensor(i_temp), his_item]))\n",
    "        edge_i.append(graph_i.edges['by'].data[dgl.NID])\n",
    "        edge_u.append(graph_u.edges['pby'].data[dgl.NID])\n",
    "    all_edge_u = torch.unique(torch.cat(edge_u))\n",
    "    all_edge_i = torch.unique(torch.cat(edge_i))\n",
    "    fin_graph = dgl.edge_subgraph(sub_graph, edges={'by':all_edge_i,'pby':all_edge_u})\n",
    "    target = test_items[j+1]\n",
    "    last_item = test_items[j]\n",
    "    u_alis = torch.where(fin_graph.nodes['user'].data['user_id']==test_user)[0]\n",
    "    last_alis = torch.where(fin_graph.nodes['item'].data['item_id']==last_item)[0]\n",
    "    user = test_user\n",
    "    user = torch.tensor([user])\n",
    "    target = torch.tensor([target])\n",
    "    graphs.append((fin_graph, u_alis, last_alis, target, user))\n",
    "\n",
    "asd = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Graph(num_nodes={'item': 269, 'user': 868},\n",
      "      num_edges={('item', 'by', 'user'): 371, ('user', 'pby', 'item'): 1244},\n",
      "      metagraph=[('item', 'user', 'by'), ('user', 'item', 'pby')]), tensor([32]), tensor([45]), tensor([9323]), tensor([14326]))]\n"
     ]
    }
   ],
   "source": [
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DGSR(user_num=89024, item_num=17676, input_dim=50, item_max_length=50,\n",
    "#              user_max_length=50, feat_drop=0.3, attn_drop=0.3, user_long='orgat', user_short='att',\n",
    "#              item_long='orgat', item_short='att', user_update='rnn', item_update='rnn', last_item=False,\n",
    "#              layer_num=2).cuda()\n",
    "\n",
    "model = DSGR(89024, 17676, 50, feat_drop=0.3, attn_drop=0.3).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"user_embedding.weight\": \"user_emb.weight\",\n",
    "    \"item_embedding.weight\": \"recipe_emb.weight\",\n",
    "    \"layers.0.agg_gate_u.weight\": \"dsgr_layers.0.agg_gate_user.weight\",\n",
    "    \"layers.0.agg_gate_i.weight\": \"dsgr_layers.0.agg_gate_recipe.weight\",\n",
    "    \"layers.0.user_weight.weight\": \"dsgr_layers.0.user_weight.weight\",\n",
    "    \"layers.0.item_weight.weight\": \"dsgr_layers.0.recipe_weight.weight\",\n",
    "    \"layers.0.user_update.weight\": \"dsgr_layers.0.user_update.weight\",\n",
    "    \"layers.0.item_update.weight\": \"dsgr_layers.0.recipe_update.weight\",\n",
    "    \"layers.0.last_weight_u.weight\": \"dsgr_layers.0.user_last_weight.weight\",\n",
    "    \"layers.0.last_weight_i.weight\": \"dsgr_layers.0.recipe_last_weight.weight\",\n",
    "    \"layers.0.i_time_encoding.weight\": \"dsgr_layers.0.recipe_date_emb.weight\",\n",
    "    \"layers.0.i_time_encoding_k.weight\": \"dsgr_layers.0.recipe_date_emb_k.weight\",\n",
    "    \"layers.0.u_time_encoding.weight\": \"dsgr_layers.0.user_date_emb.weight\",\n",
    "    \"layers.0.u_time_encoding_k.weight\": \"dsgr_layers.0.user_date_emb_k.weight\",\n",
    "    \"layers.1.agg_gate_u.weight\": \"dsgr_layers.1.agg_gate_user.weight\",\n",
    "    \"layers.1.agg_gate_i.weight\": \"dsgr_layers.1.agg_gate_recipe.weight\",\n",
    "    \"layers.1.user_weight.weight\": \"dsgr_layers.1.user_weight.weight\",\n",
    "    \"layers.1.item_weight.weight\": \"dsgr_layers.1.recipe_weight.weight\",\n",
    "    \"layers.1.user_update.weight\": \"dsgr_layers.1.user_update.weight\",\n",
    "    \"layers.1.item_update.weight\": \"dsgr_layers.1.recipe_update.weight\",\n",
    "    \"layers.1.last_weight_u.weight\": \"dsgr_layers.1.user_last_weight.weight\",\n",
    "    \"layers.1.last_weight_i.weight\": \"dsgr_layers.1.recipe_last_weight.weight\",\n",
    "    \"layers.1.i_time_encoding.weight\": \"dsgr_layers.1.recipe_date_emb.weight\",\n",
    "    \"layers.1.i_time_encoding_k.weight\": \"dsgr_layers.1.recipe_date_emb_k.weight\",\n",
    "    \"layers.1.u_time_encoding.weight\": \"dsgr_layers.1.user_date_emb.weight\",\n",
    "    \"layers.1.u_time_encoding_k.weight\": \"dsgr_layers.1.user_date_emb_k.weight\",\n",
    "    \"unified_map.weight\": \"unified_map.weight\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_state_dict = torch.load(\"./model_8.pt\")\n",
    "\n",
    "# Create a new state_dict for the model you want to load\n",
    "new_state_dict = {}\n",
    "\n",
    "# Update the state_dict keys using the mapping\n",
    "for key, value in saved_state_dict.items():\n",
    "    if key in mapping:\n",
    "        new_state_dict[mapping[key]] = value\n",
    "\n",
    "# Load the updated state_dict into your model\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_gpu = []\n",
    "for i in range(len(graphs)):\n",
    "    graphs_gpu.append((graphs[i][0].to('cuda'), graphs[i][1].to('cuda'), graphs[i][2].to('cuda'), graphs[i][3].to('cuda'), graphs[i][4].to('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'item': 269, 'user': 868},\n",
      "      num_edges={('item', 'by', 'user'): 371, ('user', 'pby', 'item'): 1244},\n",
      "      metagraph=[('item', 'user', 'by'), ('user', 'item', 'pby')])\n",
      "tensor([32], device='cuda:0')\n",
      "tensor([45], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "graph = graphs_gpu[0][0]\n",
    "user = graphs_gpu[0][1]\n",
    "last_item = graphs_gpu[0][2]\n",
    "\n",
    "print(graph)\n",
    "print(user)\n",
    "print(last_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = model(graph, user, last_alis, is_training=True)\n",
    "\n",
    "scores = []\n",
    "for i in range(len(graphs_gpu)):\n",
    "    score = model(graphs_gpu[i][0], graphs_gpu[i][1], graphs_gpu[i][2], training=True)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each score in scores, find the top 10 items then get the average of the top 10 items\n",
    "top_10 = []\n",
    "for i in range(len(scores)):\n",
    "    top_10.append(torch.topk(scores[i], 10)[1])\n",
    "    \n",
    "    \n",
    "#detaching the tensor from gpu and converting it to numpy\n",
    "top_10 = [i.cpu().detach().numpy() for i in top_10]\n",
    "\n",
    "top_dict = {}\n",
    "\n",
    "for i in range(len(top_10)):\n",
    "    for j in range(len(top_10[i][0])):\n",
    "        if top_10[i][0][j] not in top_dict:\n",
    "            top_dict[top_10[i][0][j]] = 1\n",
    "        else:\n",
    "            top_dict[top_10[i][0][j]] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3490, 1), (2387, 1), (6904, 1), (2293, 1), (3006, 1), (4787, 1), (6018, 1), (11243, 1), (11218, 1), (10150, 1)]\n"
     ]
    }
   ],
   "source": [
    "#sort the dictionary by value\n",
    "sorted_dict = sorted(top_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([4.2394, 4.1812, 4.0032, 3.9885, 3.7544, 3.7251, 3.5824, 3.5659, 3.5402,\n",
      "        3.5073], device='cuda:0', grad_fn=<TopkBackward0>),\n",
      "indices=tensor([ 3490,  2387,  6904,  2293,  3006,  4787,  6018, 11243, 11218, 10150],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "#squeeze the score\n",
    "score = score.squeeze()\n",
    "\n",
    "#print top 10 scores\n",
    "print(score.topk(10))\n",
    "\n",
    "#get the top 10 items\n",
    "top_10 = torch.topk(score, 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 items:  [ 3490  2387  6904  2293  3006  4787  6018 11243 11218 10150]\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 items: ', top_10.indices.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
